# Zotero-Arxiv-Feishu-LLM Project Analysis

## Executive Summary

**Zotero-Arxiv-Feishu-LLM** is an intelligent academic paper recommendation system that automatically fetches new arXiv papers, matches them against a user's personal Zotero library using embedding-based similarity, enriches them with LLM-generated summaries and translations, and delivers personalized recommendations via Feishu (Lark) or WeChat Work messaging platforms.

**Key Value Proposition**: Automatically curate and deliver the most relevant new research papers to researchers based on their existing research interests, reducing information overload and saving hours of manual paper screening.

---

## System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Zotero Library â”‚ â—„â”€â”€â”€ User's Research Profile
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Interest Profile Builder     â”‚
    â”‚  (titles, abstracts, authors,   â”‚
    â”‚    tags, collections)           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   arXiv Paper Fetcher           â”‚
    â”‚   - RSS Feed (delayed)          â”‚
    â”‚   - API (real-time)             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Embedding-based Similarity     â”‚
    â”‚  Ranking Engine                 â”‚
    â”‚  (GIST-small-Embedding-v0)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    LLM Enrichment Layer         â”‚
    â”‚  - TLDR Generation              â”‚
    â”‚  - Abstract Translation         â”‚
    â”‚  - Relevance Scoring            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Message Delivery System        â”‚
    â”‚  - Feishu Interactive Cards     â”‚
    â”‚  - WeChat Work Markdown         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Collection Phase**: 
   - Fetch user's Zotero library (papers with abstracts only)
   - Fetch latest arXiv papers based on configured categories

2. **Matching Phase**:
   - Generate embeddings for both Zotero papers and arXiv papers
   - Calculate cosine similarity between each arXiv paper and the Zotero corpus
   - Rank arXiv papers by average similarity score

3. **Enrichment Phase**:
   - Generate Chinese TLDR summaries (configurable language)
   - Translate abstracts to Chinese (configurable)
   - Add star ratings based on similarity scores

4. **Delivery Phase**:
   - Format results as Feishu interactive cards OR WeChat Work markdown
   - Send via webhook to configured platform

---

## Core Components

### 1. Zotero Client (`zotero_client.py`)

**Purpose**: Interface with Zotero API to fetch user's research library

**Key Features**:
- Supports both user and group libraries
- Filters by item types (conference papers, journal articles, preprints)
- Extracts: title, abstract, authors, tags, collections, links
- Skips entries without abstracts (critical for similarity matching)
- Handles DOI and URL resolution

**Implementation Highlights**:
```python
def fetch_papers(library_id, api_key, library_type, item_types, max_items)
```
- Uses `pyzotero` library for API interactions
- Smart pagination with optional item limit for large libraries
- Collection name resolution for better metadata
- Robust author name parsing (handles multiple formats)

### 2. arXiv Fetcher (`arxiv_fetcher.py`)

**Purpose**: Retrieve new academic papers from arXiv

**Key Features**:
- **Dual Mode Operation**:
  - **RSS Mode**: Uses arXiv RSS feeds (may have delays, but reliable for "new" papers)
  - **API Mode**: Direct API queries (fresher results, more flexible queries)
- Smart query normalization (converts RSS-style to API-style queries)
- Time-windowed filtering (supports fractional days for hour-level precision)
- Automatic retry logic for RSS feeds waiting for daily updates
- Version suffix removal for clean arXiv IDs

**Configuration Examples**:
```yaml
# RSS-style (simple)
arxiv.query: "cs.AI+cs.LG+cs.CV"

# API-style (advanced)
arxiv.query: "cat:cs.AI OR cat:cs.LG"
```

**Implementation Highlights**:
- RSS polling with configurable wait/retry periods
- Deduplication across multiple categories
- Date cutoff filtering for "only_new" mode
- Batch fetching (20 IDs per request) for efficiency

### 3. Similarity Engine (`similarity.py`)

**Purpose**: Rank arXiv papers by relevance to user's research interests

**Algorithm**:
1. Encode all Zotero abstracts using sentence-transformers
2. Encode all arXiv candidate abstracts
3. Compute cosine similarity matrix (candidates Ã— corpus)
4. Average similarity scores across entire Zotero corpus
5. Sort and return top-k matches

**Key Features**:
- Uses `avsolatorio/GIST-small-Embedding-v0` by default (efficient, good quality)
- L2-normalized embeddings for direct cosine similarity via dot product
- Configurable corpus size limit (`max_corpus`) for performance
- Returns enriched papers with similarity scores (0-1 range)

**Performance Considerations**:
- Batch encoding for efficiency
- Linear algebra operations via NumPy
- Memory efficient for typical use cases (<1000 papers)

### 4. LLM Utilities (`llm_utils.py`)

**Purpose**: Enhance papers with AI-generated content

**Capabilities**:

1. **Relevance Scoring** (`score` method):
   - Evaluates paper relevance to a free-form query
   - Returns structured JSON: {match: bool, score: float, reason: str}
   - Context-aware (considers title, abstract, tags, collections)

2. **Translation** (`translate` method):
   - Translates abstracts to target language (default: Chinese)
   - Direct translation approach with terminology preservation
   - Fallback to empty string on errors

3. **Summarization** (`summarize` method):
   - Generates concise TLDRs in target language
   - Configurable length (default: 80 words)
   - Focuses on: task, methods, key contributions, results
   - Avoids generic filler content

**OpenAI Compatibility**:
- Works with official OpenAI API
- Works with Azure OpenAI
- Works with self-hosted compatible endpoints (vLLM, LocalAI, etc.)
- Uses `response_format={"type": "json_object"}` for structured outputs

**Error Handling**:
- Graceful degradation (returns empty strings on failures)
- Conservative defaults for parsing errors
- JSON parsing with fallback

### 5. Feishu Integration (`feishu.py`)

**Purpose**: Deliver results as rich interactive cards to Feishu (Lark)

**Card Features**:
- **Header**: Customizable title and color template
- **Summary Section**: Total count with cute emojis
- **Paper List**: Each paper shows:
  - Title (linked)
  - Star rating (â­Ã—5 scale based on similarity)
  - Relevance score (0.00-1.00)
  - Authors (truncated if >5)
  - Keywords (top 6 tags)
  - TLDR or translated/original abstract

**Color Templates**:
- blue, wathet, turquoise, green, yellow, orange, red, carmine
- Custom hex colors auto-mapped to wathet

**Implementation Highlights**:
- Markdown formatting within Feishu card elements
- Link shortening for cleaner display
- Horizontal rule separators between papers
- Wide screen mode enabled

### 6. WeChat Work Integration (`wechat.py`)

**Purpose**: Deliver results as Markdown messages to WeChat Work (ä¼ä¸šå¾®ä¿¡)

**Challenges & Solutions**:

**Challenge 1: Message Length Limit (4096 chars)**
- Solution: Automatic message splitting
- Split threshold: 1000 chars per message (safe margin)
- Continuation headers for multi-message sequences
- Smart paper boundary detection (never splits mid-paper)

**Challenge 2: Single Message Too Long**
- Solution: Progressive abstract truncation
- Tries: 600 â†’ 400 â†’ 250 â†’ 150 â†’ 100 chars
- Final fallback: Hard truncation with indicator

**Message Structure**:
- Summary message (first): Total count, date, emoji greeting
- Individual papers: Title, stars, score, authors, keywords, TLDR/abstract
- Delay between messages (0.5s default) to avoid rate limiting

**Error Handling**:
- Per-message error catching (continues on failure)
- Length validation at multiple stages
- Truncation warnings in logs

---

## Configuration System

### Hierarchy
```
Environment Variables > config.yaml > config.example.yaml
```

### Critical Configuration Points

#### 1. **Notification Platforms** (mutually exclusive in practice)
```yaml
# Feishu (priority: low if WeChat configured)
feishu:
  webhook_url: "https://open.feishu.cn/..."
  title: "æ¯æ—¥è®ºæ–‡æ¨é€"
  header_template: "blue"

# WeChat Work (priority: high)
wechat:
  webhook_url: "https://qyapi.weixin.qq.com/..."
  title: "æ¯æ—¥è®ºæ–‡æ¨é€"
```

**Priority Logic**: WeChat Work takes precedence if both configured

#### 2. **Zotero Configuration**
```yaml
zotero:
  library_id: "1234567"          # User ID or Group ID
  api_key: "your-key"
  library_type: "user"           # "user" or "group"
  item_types:                     # Filter by type
    - conferencePaper
    - journalArticle
    - preprint
  max_items: 100                  # Performance optimization
```

#### 3. **arXiv Fetching**
```yaml
arxiv:
  query: "cs.AI+cs.RO+cs.LG"     # Categories or API query
  source: "rss"                   # "rss" or "api"
  max_results: 250
  only_new: true                  # Filter by announce_type
  days_back: 0.5                  # 0.5 = 12 hours
  rss_wait_minutes: 360           # Wait for feed update
  rss_retry_minutes: 30           # Poll interval
```

**Use Cases**:
- `source: "rss"` - Most reliable for "new" papers, may delay
- `source: "api"` - Faster updates, less filtering by "new" status
- `days_back: 0.5` - Fetch last 12 hours (for frequent checks)
- `rss_wait_minutes: 360` - Wait up to 6 hours for RSS update

#### 4. **LLM Configuration**
```yaml
llm:
  model: "gpt-4o-mini"
  base_url: "https://api.openai.com/v1"
  api_key: "sk-..."
  temperature: 0.0                # Deterministic output
```

**Compatible Providers**:
- OpenAI (official)
- Azure OpenAI
- Self-hosted (vLLM, Ollama, LocalAI)
- Any OpenAI-compatible API gateway

#### 5. **Embedding Configuration**
```yaml
embedding:
  model: "avsolatorio/GIST-small-Embedding-v0"
```

**Model Characteristics**:
- Size: ~33M parameters (small, fast)
- Architecture: Sentence-BERT variant
- Performance: Good for short texts (titles, abstracts)
- Local execution via sentence-transformers library

#### 6. **Query/Output Configuration**
```yaml
query:
  max_results: 10                # Papers to deliver
  max_corpus: 250                # Zotero papers for matching
  include_abstract: true
  translate_abstract: true
  include_tldr: true
  tldr_language: "Chinese"
  tldr_max_words: 80
```

### Environment Variables

**Required**:
- `ZOTERO_ID`, `ZOTERO_KEY`, `ZOTERO_LIBRARY_TYPE`
- `LLM_API_KEY`, `LLM_MODEL`, `LLM_BASE_URL`
- `FEISHU_WEBHOOK` or `WECHAT_WEBHOOK` (at least one)

**Aliases**:
- `OPENAI_*` â†’ `LLM_*` (for compatibility)
- `LARK_WEBHOOK` â†’ `FEISHU_WEBHOOK`
- `WECHAT_WORK_WEBHOOK` â†’ `WECHAT_WEBHOOK`

**Testing**:
- `FEISHU_TEST_WEBHOOK` - Safe testing without disrupting production
- `WECHAT_TEST_WEBHOOK` - Safe testing for WeChat Work

---

## GitHub Actions Automation

### Workflows

#### 1. **Feishu Production** (`.github/workflows/run.yml`)
- **Trigger**: Cron schedule `0 0 * * 1-5` (Mon-Fri at midnight UTC)
- **Manual**: `workflow_dispatch` enabled
- **Timeout**: 360 minutes (6 hours for RSS waiting)
- **Secrets**: FEISHU_WEBHOOK, ZOTERO_*, LLM_*

#### 2. **WeChat Work Production** (`.github/workflows/run_ep_wechat.yml`)
- Similar to Feishu workflow but uses WECHAT_WEBHOOK

#### 3. **Test Workflows**
- `test.yml` - Feishu testing with `FEISHU_TEST_WEBHOOK`
- `test_ep_wechat.yml` - WeChat Work testing with `WECHAT_TEST_WEBHOOK`
- Manual trigger only

### Setup Instructions
1. Fork the repository
2. Go to Settings â†’ Secrets and variables â†’ Actions
3. Add required secrets (see Environment Variables above)
4. Enable GitHub Actions in Actions tab
5. Manually trigger test workflow to verify
6. Adjust `config.example.yaml` for your categories
7. Let scheduled workflow run automatically

**Zero Local Setup**: Entirely cloud-based execution possible

---

## Technical Deep Dives

### Embedding Similarity Algorithm

**Why Embeddings?**
- Captures semantic meaning beyond keyword matching
- Understands synonyms, related concepts
- Works across languages (to some extent)
- More robust than TF-IDF or BM25 for academic text

**Process**:
```python
# 1. Encode corpus (Zotero)
corpus_emb = model.encode([p["abstract"] for p in corpus])

# 2. Encode candidates (arXiv)
cand_emb = model.encode([p["abstract"] for p in candidates])

# 3. Compute similarity matrix
scores = cand_emb @ corpus_emb.T  # Shape: (n_candidates, n_corpus)

# 4. Average across corpus
avg_scores = scores.mean(axis=1)  # Shape: (n_candidates,)

# 5. Sort and select top-k
ranked = sorted(zip(candidates, avg_scores), key=lambda x: -x[1])[:k]
```

**Optimization**:
- Normalized embeddings â†’ cosine similarity via dot product
- Batch encoding (not per-paper)
- NumPy for vectorized operations
- `max_corpus` cap to control compute cost

### LLM Prompt Engineering

**Relevance Scoring Prompt** (Chinese, strict JSON):
```
ä½ æ˜¯èµ„æ·±å­¦æœ¯åŠ©æ‰‹ï¼Œéœ€è¯„ä¼°ä¸€ç¯‡è®ºæ–‡ä¸ç”¨æˆ·éœ€æ±‚çš„ç›¸å…³æ€§ï¼Œå¹¶ç»™å‡ºç®€çŸ­ç†ç”±ã€‚
ç”¨æˆ·éœ€æ±‚: {query}
è®ºæ–‡å…ƒä¿¡æ¯ï¼š
- æ ‡é¢˜: {title}
- æ‘˜è¦: {summary}
- æ ‡ç­¾: {tags}
- é›†åˆ: {collections}

è¾“å‡ºä¸¥æ ¼çš„ JSONï¼ˆä»…ä¸€è¡Œï¼‰ï¼š
{"match": true/false, "score": 0.00, "reason": "ä¸­æ–‡ç†ç”±ï¼Œâ‰¤30å­—"}
è§„åˆ™ï¼š
- score åœ¨ 0-1ï¼Œ0=å®Œå…¨ä¸ç›¸å…³æˆ–ä¿¡æ¯ä¸è¶³ï¼Œ1=é«˜åº¦å¥‘åˆ
- å…³æ³¨ä¸»é¢˜/æ–¹æ³•/åº”ç”¨åœºæ™¯çš„åŒ¹é…åº¦ï¼Œé¿å…ä»…å‡­å…³é”®è¯
- reason åªå†™æ ¸å¿ƒåŒ¹é…/ä¸åŒ¹é…ç‚¹
```

**TLDR Prompt**:
```
ç”¨{target_lang}å†™ä¸€ä¸ªç²¾ç‚¼ TLDRï¼ˆçº¦{max_words}è¯ï¼‰ï¼Œ
çªå‡ºä»»åŠ¡ã€æ–¹æ³•ã€å…³é”®è´¡çŒ®ä¸ä¸»è¦ç»“æœï¼Œé¿å…å£æ°´è¯ï¼š
æ ‡é¢˜: {title}
æ‘˜è¦: {abstract}
```

**Translation Prompt**:
```
è¯·å°†ä»¥ä¸‹æ‘˜è¦ç¿»è¯‘ä¸º{target_lang}ï¼Œç›´è¯‘ä¸ºä¸»ï¼Œä¿æŒæœ¯è¯­å‡†ç¡®ï¼Œ
é¿å…æ·»åŠ è¯´æ˜ï¼Œç›´æ¥è¾“å‡ºè¯‘æ–‡ï¼š
{text}
```

**Design Principles**:
- Clear role definition ("èµ„æ·±å­¦æœ¯åŠ©æ‰‹")
- Explicit output format (JSON schema)
- Strict length constraints (â‰¤30å­—, ~80è¯)
- Focus on substance over style
- Temperature=0.0 for consistency

### WeChat Work Message Splitting Algorithm

**Challenge**: 4096 character limit per message

**Algorithm**:
```python
MAX_LENGTH = 1000  # Conservative limit (with 3096 char safety margin)

messages = []
current_message = [header]
current_length = len(header)

for paper in papers:
    paper_content = format_paper(paper)
    
    if current_length + len(paper_content) > MAX_LENGTH:
        # Save current message
        messages.append("".join(current_message))
        
        # Start new message
        current_message = [continuation_header, paper_content]
        current_length = len(continuation_header) + len(paper_content)
    else:
        # Add to current message
        current_message.append(paper_content)
        current_length += len(paper_content)

# Add last message
messages.append("".join(current_message))
```

**Safety Measures**:
1. Conservative split threshold (1000 vs 4096)
2. Progressive abstract truncation (600â†’400â†’250â†’150â†’100)
3. Final hard truncation with marker
4. Multiple validation stages
5. Truncation indicators ("*ï¼ˆå†…å®¹è¿‡é•¿å·²æˆªæ–­ï¼‰*")

**Trade-offs**:
- More messages vs complete content
- User experience (multiple notifications)
- Rate limiting considerations (0.5s delay between messages)

---

## Performance Characteristics

### Execution Time Analysis

**Typical Run** (10 Zotero papers, 50 arXiv papers, 5 results):
1. Zotero fetch: ~2-5 seconds
2. arXiv fetch (RSS): ~5-10 seconds (or up to hours if waiting)
3. arXiv fetch (API): ~3-8 seconds
4. Embedding encoding: ~5-15 seconds (CPU)
5. Similarity computation: <1 second
6. LLM enrichment: ~10-30 seconds (5 papers Ã— 2-6s each)
7. Message delivery: ~1-2 seconds

**Total**: ~30-70 seconds (normal), up to 6 hours (RSS wait mode)

### Scalability Limits

**Current Design**:
- **Zotero corpus**: 100-500 papers recommended, 1000+ possible but slower
- **arXiv candidates**: 50-500 papers typical
- **LLM calls**: 5-20 papers feasible (cost and time)
- **Message size**: Auto-splitting handles any number of results

**Bottlenecks**:
1. Embedding encoding (linear in paper count)
2. LLM API latency (linear in result count, no batching)
3. RSS feed delay (external, uncontrollable)

**Optimization Strategies**:
- `max_corpus`: Cap Zotero papers for similarity
- `max_items`: Limit Zotero fetch
- `max_results`: Reduce final output
- Embedding model: Switch to smaller/faster model
- LLM batching: Not currently implemented

### Cost Analysis

**API Costs** (per run):

**Embedding** (local, free):
- Model: GIST-small (33M params)
- Runs on CPU, no API costs
- ~5-15s on typical cloud runner

**LLM Costs** (example: GPT-4o-mini):
- TLDR: ~300-500 tokens input, ~100-200 tokens output per paper
- Translation: ~200-400 tokens input, ~200-500 tokens output per paper
- Score: ~200-300 tokens input, ~50-100 tokens output per paper

**Rough Estimate** (10 papers/day):
- Input: ~(300+200+200)Ã—10 = 7,000 tokens
- Output: ~(150+350+75)Ã—10 = 5,750 tokens
- Cost at $0.15/1M input, $0.60/1M output:
  - ~$0.001 input + ~$0.003 output = **~$0.004/run**
  - **~$1.20/year** (daily runs, 300 days)

**Zotero API**: Free (rate limited)
**arXiv API**: Free (rate limited)

---

## Use Cases & Workflows

### Use Case 1: Academic Researcher
**Profile**: PhD student in Computer Vision
**Goal**: Stay current with latest CV papers without drowning in noise

**Setup**:
1. Maintains Zotero library with ~200 papers in CV subfields
2. Configures `arxiv.query: "cs.CV+cs.LG+cs.AI"`
3. Sets `query.max_results: 10`
4. Enables TLDR and translation (native language: Chinese)
5. Daily delivery at 8 AM local time (via GitHub Actions cron)

**Outcome**:
- Receives 10 most relevant new CV papers every morning
- Chinese TLDRs allow quick scanning
- Star ratings indicate relevance
- Clicks through to arXiv for interesting papers
- **Time saved**: ~30-60 min/day of manual scanning

### Use Case 2: Research Lab Group
**Profile**: 10-person lab in Robotics
**Goal**: Shared awareness of relevant new research

**Setup**:
1. Group Zotero library with ~500 papers (collective interests)
2. Configures `zotero.library_type: "group"`
3. Mixed query: `arxiv.query: "cs.RO+cs.AI+cs.CV"`
4. WeChat Work group bot (team uses WeChat Work)
5. Scheduled runs Mon-Fri

**Outcome**:
- Daily digest in team chat
- Sparks discussions ("Did you see paper #3?")
- Collective knowledge building
- Junior members learn from senior members' interests (via Zotero)

### Use Case 3: Industry Research Team
**Profile**: Corporate AI research division
**Goal**: Monitor competitors and emerging techniques

**Setup**:
1. Curated Zotero library of strategic interest areas
2. Self-hosted LLM (for data privacy)
3. Feishu bot (corporate communication platform)
4. Narrow, high-precision query
5. High relevance threshold (only top 5 results)

**Outcome**:
- Competitive intelligence without manual search
- Privacy maintained (self-hosted LLM)
- Executives see high-signal summaries
- Strategic planning informed by research trends

---

## Strengths & Limitations

### Strengths

1. **Minimal Friction**: 
   - No complex ML infrastructure needed
   - Remote APIs for heavy lifting (LLM, optional for embedding)
   - GitHub Actions = zero server management

2. **Personalization**:
   - Zotero library = implicit interest profile
   - No manual query tuning needed
   - Evolves as library grows

3. **Quality Filtering**:
   - Embedding similarity > keyword matching
   - LLM summaries > raw abstracts
   - Star ratings for quick triage

4. **Flexible Delivery**:
   - Feishu cards (rich formatting)
   - WeChat Work (widespread in China)
   - Auto-splitting for length limits

5. **Open Source & Forkable**:
   - Easy customization
   - No vendor lock-in
   - Community contributions welcome

### Limitations

1. **Zotero Dependency**:
   - Requires existing Zotero library
   - Quality depends on library comprehensiveness
   - Cold start problem for new users

2. **arXiv Only**:
   - Doesn't cover journals, conferences after publication
   - Limited to arXiv-hosted fields (CS, Physics, Math, etc.)
   - No coverage of non-English papers (outside arXiv)

3. **LLM Costs**:
   - Scales with result count
   - Requires API access
   - Self-hosted models may lack JSON support

4. **Embedding Limitations**:
   - Only uses abstracts (ignores full text)
   - English-centric models (may not work well for non-English)
   - Doesn't capture very new concepts (model training lag)

5. **No User Feedback Loop**:
   - Can't learn from clicks or ratings
   - Static similarity metric
   - No adaptive ranking

6. **GitHub Actions Constraints**:
   - Public repos only (for free tier)
   - Runner time limits (6 hours)
   - Rate limiting on API calls

---

## Extension Ideas

### Short-term Enhancements

1. **Multi-source Support**:
   - Add bioRxiv, medRxiv, SSRN
   - Aggregate across sources
   - Unified deduplication

2. **Feedback Collection**:
   - Add "ğŸ‘/ğŸ‘" buttons to Feishu cards
   - Log feedback to GitHub Issues or database
   - Future: Use feedback for re-ranking

3. **Digest Modes**:
   - Daily full digest (current)
   - Weekly summary (top 20-30 papers)
   - Monthly highlights (highest-scoring papers)

4. **Smart Scheduling**:
   - Detect if no new papers â†’ skip notification
   - Adjust timing based on arXiv submission patterns
   - Timezone-aware delivery

5. **Enhanced Filtering**:
   - Author whitelist/blacklist
   - Institution filtering
   - Citation count thresholds (if available)

### Long-term Vision

1. **ML-Enhanced Ranking**:
   - Train a learning-to-rank model on user feedback
   - Combine embedding similarity with other features (author reputation, venue, citation velocity)
   - Personalized re-ranker per user

2. **Full-Text Analysis**:
   - Download and parse PDFs
   - Extract figures, tables, code snippets
   - Summarize methodology sections separately

3. **Conversational Interface**:
   - Slack bot with Q&A
   - "Tell me more about paper #3"
   - "Find papers similar to #3"

4. **Collaborative Filtering**:
   - Share anonymized preferences across users
   - "Users with similar interests also read..."
   - Community-driven discovery

5. **Citation Network Analysis**:
   - Track backward citations (references)
   - Track forward citations (future papers citing these)
   - Identify seminal papers in user's interest area

6. **Multi-Modal Summaries**:
   - Generate visual abstracts (diagrams, charts)
   - Audio summaries (TTS for commutes)
   - Video explainers (future LLM capability)

---

## Code Quality Assessment

### Strengths

1. **Clean Separation of Concerns**:
   - Each module has a single responsibility
   - No circular dependencies
   - Easy to test in isolation

2. **Robust Error Handling**:
   - Try-except blocks around external API calls
   - Fallback values (empty strings, default scores)
   - Graceful degradation

3. **Configuration Flexibility**:
   - Environment variables for secrets
   - YAML for structure
   - Sensible defaults

4. **Type Hints**:
   - Most functions have type annotations
   - Improves IDE support and readability

5. **Documentation**:
   - Comprehensive README files (EN + ZH)
   - Inline docstrings for complex functions
   - Config examples provided

### Areas for Improvement

1. **Testing**:
   - No unit tests currently
   - No integration tests
   - Manual testing only (`test_run.py`, `test_wechat.py`)

2. **Logging**:
   - Uses `print()` instead of `logging` module
   - No log levels (DEBUG, INFO, ERROR)
   - Hard to filter or redirect logs

3. **Retries & Rate Limiting**:
   - arXiv client has retries
   - Feishu/WeChat POST has no retry logic
   - No exponential backoff

4. **Input Validation**:
   - Minimal validation of config values
   - Could crash on malformed input
   - No schema validation (e.g., pydantic)

5. **Async Opportunities**:
   - LLM calls are sequential (could parallelize)
   - Embedding encoding is synchronous
   - Message delivery could be async

6. **Dependency Pinning**:
   - `requirements.txt` uses `>=` (not exact pins)
   - Could lead to version conflicts
   - No `requirements-dev.txt` for testing

### Recommended Refactors

1. **Add pytest suite**:
   - Mock Zotero/arXiv APIs
   - Test similarity ranking
   - Test message splitting logic

2. **Replace print with logging**:
   ```python
   import logging
   logger = logging.getLogger(__name__)
   logger.info(f"Fetched {len(papers)} papers")
   ```

3. **Add pydantic for config validation**:
   ```python
   from pydantic import BaseModel, AnyHttpUrl
   
   class ZoteroConfig(BaseModel):
       library_id: str
       api_key: str
       library_type: Literal["user", "group"]
   ```

4. **Implement retry decorator**:
   ```python
   from tenacity import retry, stop_after_attempt, wait_exponential
   
   @retry(stop=stop_after_attempt(3), wait=wait_exponential())
   def post_to_feishu(webhook_url, payload):
       ...
   ```

5. **Parallelize LLM calls**:
   ```python
   from concurrent.futures import ThreadPoolExecutor
   
   with ThreadPoolExecutor(max_workers=5) as executor:
       futures = [executor.submit(scorer.summarize, p) for p in papers]
       results = [f.result() for f in futures]
   ```

---

## Security Considerations

### Current Security Posture

**Good Practices**:
1. âœ… Secrets via environment variables (not hardcoded)
2. âœ… GitHub Secrets for CI/CD
3. âœ… HTTPS for all API calls
4. âœ… No sensitive data in logs
5. âœ… Input sanitization (via API library usage)

**Potential Risks**:
1. âš ï¸ **API Key Exposure**: 
   - If config.yaml committed with real keys
   - Mitigation: .gitignore includes config.yaml
   
2. âš ï¸ **Dependency Vulnerabilities**:
   - No automated security scanning
   - Mitigation: Manual updates, consider Dependabot
   
3. âš ï¸ **LLM Prompt Injection**:
   - User-controlled Zotero content in prompts
   - Low risk (no harmful actions possible)
   - Mitigation: Escaped strings, JSON-only output

4. âš ï¸ **Webhook URL Leakage**:
   - If printed in logs or error messages
   - Mitigation: Don't log webhook URLs

### Recommendations

1. **Add security scanning**:
   - GitHub Dependabot for dependency updates
   - CodeQL for code scanning
   - Secret scanning (already enabled on GitHub)

2. **Implement rate limiting checks**:
   - Track API call counts
   - Warn if approaching limits
   - Graceful degradation

3. **Audit Zotero access**:
   - Use read-only API keys
   - Minimal scope (library access only)
   - Rotate keys periodically

4. **Add webhook validation**:
   - Verify Feishu signature (if available)
   - WeChat Work signature validation
   - Prevent webhook spoofing

---

## Deployment Options

### 1. GitHub Actions (Recommended)
**Pros**:
- Zero infrastructure management
- Free for public repos
- Integrated with code
- Automatic on schedule

**Cons**:
- Public repos only (free tier)
- Time limits (6 hours)
- No private data handling

**Setup**: See README

### 2. Self-Hosted Runner
**Pros**:
- Private repos supported
- No time limits
- Full control

**Cons**:
- Requires server
- Maintenance burden
- Cost

**Setup**:
```bash
# Install runner
./config.sh --url https://github.com/user/repo
./run.sh

# Add to cron
0 0 * * * cd /path/to/repo && python main.py
```

### 3. Cloud Functions (Serverless)
**Pros**:
- Pay-per-use
- Scales to zero
- Managed infrastructure

**Cons**:
- Cold start latency
- Vendor lock-in
- Function size limits

**Example (AWS Lambda)**:
```python
def lambda_handler(event, context):
    from main import main
    main()
    return {"statusCode": 200}
```

### 4. Docker Container
**Pros**:
- Reproducible environment
- Portable across clouds
- Easy local testing

**Cons**:
- Requires orchestration
- Image size (ML models)

**Dockerfile**:
```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
```

### 5. Kubernetes CronJob
**Pros**:
- Enterprise-grade scheduling
- Resource management
- Multi-tenant

**Cons**:
- Complexity
- Cost

**CronJob Manifest**:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: zotero-arxiv
spec:
  schedule: "0 0 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: main
            image: your-registry/zotero-arxiv:latest
            envFrom:
            - secretRef:
                name: zotero-secrets
```

---

## Conclusion

**Zotero-Arxiv-Feishu-LLM** is a well-designed, pragmatic solution to the academic information overload problem. It demonstrates:

- **Smart Architecture**: Leverages existing services (Zotero, arXiv, LLM APIs) rather than building from scratch
- **User-Centric Design**: Minimal setup, automatic personalization, flexible delivery
- **Production-Ready Code**: Error handling, configurability, documentation
- **Extensibility**: Clear module boundaries enable easy enhancements

**Best For**:
- Individual researchers who maintain Zotero libraries
- Research groups with shared knowledge bases
- Teams using Feishu or WeChat Work for communication

**Not Ideal For**:
- Users without Zotero (no existing interest profile)
- Fields not well-represented on arXiv (law, medicine, social sciences)
- Organizations requiring on-premise LLMs with JSON support

**Overall Assessment**: â­â­â­â­â­ (5/5)

A production-ready, open-source tool that solves a real problem with elegant simplicity. The code quality is high, documentation is excellent, and the system is designed for long-term maintainability. Highly recommended for anyone in the target audience.

---

## Appendix: File Structure

```
zotero-arxiv-feishu-llm/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ run.yml                    # Feishu production
â”‚       â”œâ”€â”€ run_ep_wechat.yml          # WeChat production
â”‚       â”œâ”€â”€ test.yml                   # Feishu testing
â”‚       â””â”€â”€ test_ep_wechat.yml         # WeChat testing
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ teaser.png                     # Screenshot for README
â”œâ”€â”€ arxiv_fetcher.py                   # arXiv API/RSS client
â”œâ”€â”€ config.example.yaml                # Config template
â”œâ”€â”€ feishu.py                          # Feishu card builder
â”œâ”€â”€ llm_utils.py                       # LLM client (scoring, TLDR, translate)
â”œâ”€â”€ main.py                            # Main orchestration
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ similarity.py                      # Embedding-based ranking
â”œâ”€â”€ test_run.py                        # Manual test script
â”œâ”€â”€ test_wechat.py                     # WeChat webhook test
â”œâ”€â”€ wechat.py                          # WeChat Work message builder
â”œâ”€â”€ zotero_client.py                   # Zotero API client
â”œâ”€â”€ README.md                          # English documentation
â”œâ”€â”€ README.zh.md                       # Chinese documentation
â””â”€â”€ .gitignore                         # Git ignore rules
```

**Total**: ~1,500 lines of Python code (excluding tests, docs, config)

---

## Appendix: Dependency Breakdown

| Package | Purpose | Version |
|---------|---------|---------|
| `openai` | LLM API client | >=1.12.0 |
| `pyzotero` | Zotero API client | >=1.5.18 |
| `PyYAML` | Config parsing | >=6.0 |
| `requests` | HTTP client (webhooks) | >=2.31.0 |
| `feedparser` | RSS/Atom parsing | >=6.0.11 |
| `arxiv` | arXiv API client | >=1.4.8 |
| `sentence-transformers` | Embedding models | >=2.5.1 |
| `numpy` | Numerical operations | >=1.26.0 |

**Transitive Dependencies** (via sentence-transformers):
- `torch`, `transformers`, `huggingface-hub`, etc.

**Installation Size**: ~1-2 GB (mostly PyTorch for embeddings)

---

*Analysis completed: 2026-01-28*
*Analyzed by: AI Assistant*
*Repository: DelinQu/zotero-arxiv-feishu-llm*
